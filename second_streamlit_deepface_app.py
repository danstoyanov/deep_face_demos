# --- START OF FILE second_streamlit_deepface_app.py ---
#
# --- –ó–∞ –¥–∞ —Ä—ä–Ω–Ω–µ—à –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ—Ç–æ –∏–∑–ø–æ–ª–∑–≤–∞–π —Ç–∞–∑–∏ –∫–æ–º–∞–Ω–¥–∞ –≤ bash —Ç–µ—Ä–º–∏–Ω–∞–ª–∞ ---
# üî¥ streamlit run second_streamlit_deepface_app.py
# -------------------------------------------------------------------------

import streamlit as st
from deepface import DeepFace
import cv2
import numpy as np
# PIL is imported but not explicitly used for image processing here, which is fine as cv2 handles it.
from PIL import Image
import io
import os
import tempfile
import time
# import base64 # –ú–æ–∂–µ –¥–∞ –Ω–µ –µ –Ω—É–∂–µ–Ω, –∞–∫–æ –Ω–µ –≤–≥—Ä–∞–∂–¥–∞–º–µ –≤–∏–¥–µ–æ –∫–∞—Ç–æ base64

# --- –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞—Ç–∞ ---
st.set_page_config(
    page_title="–ú—É–ª—Ç–∏—Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª–µ–Ω –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –Ω–∞ –õ–∏—Ü–∞", layout="wide")
st.title("–ú—É–ª—Ç–∏—Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª–µ–Ω –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –Ω–∞ –õ–∏—Ü–∞")
st.markdown("""
    –¢–æ–≤–∞ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –æ–±–µ–¥–∏–Ω—è–≤–∞ –Ω—è–∫–æ–ª–∫–æ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª–Ω–æ—Å—Ç–∏ –∑–∞ –∞–Ω–∞–ª–∏–∑ –Ω–∞ –ª–∏—Ü–∞:
    1.  **–ê–Ω–∞–ª–∏–∑ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ (DeepFace):** –†–∞–∑–ø–æ–∑–Ω–∞–≤–∞–Ω–µ –Ω–∞ –≤—ä–∑—Ä–∞—Å—Ç, –ø–æ–ª –∏ –µ–º–æ—Ü–∏—è.
    2.  **–°—Ä–∞–≤–Ω—è–≤–∞–Ω–µ –Ω–∞ –¥–≤–µ –ª–∏—Ü–∞ (InsightFace):** –ö–∞—á–≤–∞–Ω–µ –Ω–∞ –¥–≤–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ –∏–∑—á–∏—Å–ª—è–≤–∞–Ω–µ –Ω–∞ —Å—Ö–æ–¥—Å—Ç–≤–æ—Ç–æ –º–µ–∂–¥—É –ª–∏—Ü–∞—Ç–∞.
    3.  **–ê–Ω–∞–ª–∏–∑ –Ω–∞ –µ–º–æ—Ü–∏–∏ –≤—ä–≤ –≤–∏–¥–µ–æ (DeepFace):** –ö–∞—á–≤–∞–Ω–µ –Ω–∞ –≤–∏–¥–µ–æ –∏ –∞–Ω–∞–ª–∏–∑ –Ω–∞ –µ–º–æ—Ü–∏–∏—Ç–µ –∫–∞–¥—ä—Ä –ø–æ –∫–∞–¥—ä—Ä.
    4.  **–ê–Ω–∞–ª–∏–∑ –æ—Ç —É–µ–± –∫–∞–º–µ—Ä–∞ (Real-time):** –†–∞–∑–ø–æ–∑–Ω–∞–≤–∞–Ω–µ –Ω–∞ –≤—ä–∑—Ä–∞—Å—Ç, –ø–æ–ª –∏ –µ–º–æ—Ü–∏—è –≤ —Ä–µ–∞–ª–Ω–æ –≤—Ä–µ–º–µ.
""")

# --- –û–±—â–∏ –ø–æ–º–æ—â–Ω–∏ —Ñ—É–Ω–∫—Ü–∏–∏ ---
MAX_DISPLAY_DIM = 600


@st.cache_data
def resize_image_for_display(image_np, max_dim):
    """
    –ü—Ä–µ–æ—Ä–∞–∑–º–µ—Ä—è–≤–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ, –∑–∞ –¥–∞ —Å–µ –ø–æ–±–µ—Ä–µ –≤ –º–∞–∫—Å–∏–º–∞–ª–Ω–∏ —Ä–∞–∑–º–µ—Ä–∏ –∑–∞ –¥–∏—Å–ø–ª–µ–π,
    –∑–∞–ø–∞–∑–≤–∞–π–∫–∏ –ø—Ä–æ–ø–æ—Ä—Ü–∏–∏—Ç–µ.
    """
    height, width = image_np.shape[:2]
    if width <= max_dim and height <= max_dim:
        return image_np
    scale = max_dim / max(width, height)
    new_width = int(width * scale)
    new_height = int(height * scale)
    resized_image = cv2.resize(
        image_np, (new_width, new_height), interpolation=cv2.INTER_AREA)
    return resized_image

# --- –ö–µ—à–∏—Ä–∞–Ω–µ –Ω–∞ –º–æ–¥–µ–ª–∏ ---
# –ú–æ–¥–µ–ª–∏—Ç–µ —Å–µ –∑–∞—Ä–µ–∂–¥–∞—Ç —Å–∞–º–æ –≤–µ–¥–Ω—ä–∂ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–∏—Ä–∞–Ω–µ –Ω–∞ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ—Ç–æ –∏–ª–∏ –ø—ä—Ä–≤–æ—Ç–æ –∏–º –∏–∑–ø–æ–ª–∑–≤–∞–Ω–µ –≤ —Å–µ—Å–∏—è—Ç–∞,
# –±–ª–∞–≥–æ–¥–∞—Ä–µ–Ω–∏–µ –Ω–∞ st.cache_resource.


@st.cache_resource
def load_deepface_models():
    """–ó–∞—Ä–µ–∂–¥–∞ DeepFace –º–æ–¥–µ–ª–∏—Ç–µ –∏ –≥–∏ –∫–µ—à–∏—Ä–∞."""
    st.info("–ó–∞—Ä–µ–∂–¥–∞–Ω–µ/—Å–≤–∞–ª—è–Ω–µ –Ω–∞ DeepFace –º–æ–¥–µ–ª–∏ (–º–æ–∂–µ –¥–∞ –æ—Ç–Ω–µ–º–µ –≤—Ä–µ–º–µ)...")
    try:
        # –ò–∑–ø—ä–ª–Ω—è–≤–∞–º–µ —Ñ–∏–∫—Ç–∏–≤–µ–Ω –∞–Ω–∞–ª–∏–∑, –∑–∞ –¥–∞ –Ω–∞–∫–∞—Ä–∞–º–µ DeepFace –¥–∞ –∑–∞—Ä–µ–¥–∏ –º–æ–¥–µ–ª–∏—Ç–µ
        dummy_image = np.zeros((100, 100, 3), dtype=np.uint8)
        DeepFace.analyze(dummy_image, actions=[
                         'age', 'gender', 'emotion'], enforce_detection=False, silent=True)
        st.success("DeepFace –º–æ–¥–µ–ª–∏—Ç–µ —Å–∞ –≥–æ—Ç–æ–≤–∏.")
        return True
    except Exception as e:
        st.error(f"–ì—Ä–µ—à–∫–∞ –ø—Ä–∏ –∑–∞—Ä–µ–∂–¥–∞–Ω–µ –Ω–∞ DeepFace –º–æ–¥–µ–ª–∏: {e}")
        return False  # –í—Ä—ä—â–∞–º–µ False, –∞–∫–æ –º–æ–¥–µ–ª–∏—Ç–µ –Ω–µ —Å–∞ –∑–∞—Ä–µ–¥–µ–Ω–∏ —É—Å–ø–µ—à–Ω–æ.


deepface_models_loaded = load_deepface_models()

try:
    import insightface
    from insightface.app import FaceAnalysis
    from sklearn.metrics.pairwise import cosine_similarity
    INSIGHTFACE_AVAILABLE = True
except ImportError:
    INSIGHTFACE_AVAILABLE = False
    st.warning("–ë–∏–±–ª–∏–æ—Ç–µ–∫–∏—Ç–µ 'insightface' –∏–ª–∏ 'scikit-learn' –Ω–µ —Å–∞ –Ω–∞–º–µ—Ä–µ–Ω–∏. –§—É–Ω–∫—Ü–∏—è—Ç–∞ –∑–∞ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –Ω–∞ –ª–∏—Ü–∞ –Ω—è–º–∞ –¥–∞ –µ –¥–æ—Å—Ç—ä–ø–Ω–∞. –ú–æ–ª—è, –∏–Ω—Å—Ç–∞–ª–∏—Ä–∞–π—Ç–µ –≥–∏: `pip install insightface onnxruntime scikit-learn`")


@st.cache_resource
def load_insightface_model():
    """–ó–∞—Ä–µ–∂–¥–∞ InsightFace –º–æ–¥–µ–ª–∞ –∏ –≥–æ –∫–µ—à–∏—Ä–∞."""
    if not INSIGHTFACE_AVAILABLE:
        return None
    st.info("–ó–∞—Ä–µ–∂–¥–∞–Ω–µ/—Å–≤–∞–ª—è–Ω–µ –Ω–∞ InsightFace –º–æ–¥–µ–ª (buffalo_l)...")
    try:
        app_insight = FaceAnalysis(name='buffalo_l', providers=[
                                   'CUDAExecutionProvider', 'CPUExecutionProvider'])
        app_insight.prepare(ctx_id=0, det_size=(640, 640))
        st.success("InsightFace –º–æ–¥–µ–ª 'buffalo_l' –µ –∑–∞—Ä–µ–¥–µ–Ω (–æ–ø–∏—Ç–≤–∞ GPU).")
        return app_insight
    except Exception as e_gpu:
        st.warning(
            f"–ù–µ—É—Å–ø–µ—à–Ω–æ –∑–∞—Ä–µ–∂–¥–∞–Ω–µ –Ω–∞ InsightFace —Å GPU ({e_gpu}). –û–ø–∏—Ç–≤–∞–º —Å–∞–º–æ —Å CPU...")
        try:
            app_insight = FaceAnalysis(name='buffalo_l', providers=[
                                       'CPUExecutionProvider'])
            app_insight.prepare(ctx_id=-1, det_size=(640, 640))
            st.success("InsightFace –º–æ–¥–µ–ª 'buffalo_l' –µ –∑–∞—Ä–µ–¥–µ–Ω —Å CPU.")
            return app_insight
        except Exception as e_cpu:
            st.error(
                f"–ì—Ä–µ—à–∫–∞ –ø—Ä–∏ –∑–∞—Ä–µ–∂–¥–∞–Ω–µ –Ω–∞ InsightFace –º–æ–¥–µ–ª —Å CPU: {e_cpu}")
            return None


# Load InsightFace model only if available
insightface_model_app = None
if INSIGHTFACE_AVAILABLE:
    insightface_model_app = load_insightface_model()


st.sidebar.title("‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∏ –†–µ–∂–∏–º–∏")

# --- –ò–∑–±–æ—Ä –Ω–∞ —Ä–µ–∂–∏–º –Ω–∞ –∞–Ω–∞–ª–∏–∑ (–æ—Å—Ç–∞–≤–∞ —Ç—É–∫, –≥–æ—Ä–µ –≤ sidebar) ---
analysis_mode = st.sidebar.selectbox(
    "–ò–∑–±–µ—Ä–µ—Ç–µ —Ä–µ–∂–∏–º –Ω–∞ –∞–Ω–∞–ª–∏–∑:",
    ("–ê–Ω–∞–ª–∏–∑ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ (DeepFace)",
     "–°—Ä–∞–≤–Ω—è–≤–∞–Ω–µ –Ω–∞ –¥–≤–µ –ª–∏—Ü–∞ (InsightFace)",
     "–ê–Ω–∞–ª–∏–∑ –Ω–∞ –µ–º–æ—Ü–∏–∏ –≤—ä–≤ –≤–∏–¥–µ–æ (DeepFace)",
     "–ê–Ω–∞–ª–∏–∑ –æ—Ç —É–µ–± –∫–∞–º–µ—Ä–∞ (Real-time)")
)

# ==============================================================================
# –†–ï–ñ–ò–ú 1: –ê–ù–ê–õ–ò–ó –ù–ê –ò–ó–û–ë–†–ê–ñ–ï–ù–ò–ï (–í–™–ó–†–ê–°–¢, –ü–û–õ, –ï–ú–û–¶–ò–Ø –° DEEPFACE)
# ==============================================================================
if analysis_mode == "–ê–Ω–∞–ª–∏–∑ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ (DeepFace)":
    st.header("1. –ê–Ω–∞–ª–∏–∑ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ (–í—ä–∑—Ä–∞—Å—Ç, –ü–æ–ª, –ï–º–æ—Ü–∏—è)")
    st.write("–ö–∞—á–µ—Ç–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ, –∑–∞ –¥–∞ –∞–Ω–∞–ª–∏–∑–∏—Ä–∞—Ç–µ –ª–∏—Ü–∞—Ç–∞ –∑–∞ –≤—ä–∑—Ä–∞—Å—Ç, –µ–º–æ—Ü–∏—è –∏ –ø–æ–ª, –∏–∑–ø–æ–ª–∑–≤–∞–π–∫–∏ DeepFace.")

    if not deepface_models_loaded:
        st.error("DeepFace –º–æ–¥–µ–ª–∏—Ç–µ –Ω–µ —Å–∞ –∑–∞—Ä–µ–¥–µ–Ω–∏. –§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª–Ω–æ—Å—Ç—Ç–∞ –Ω–µ –µ –¥–æ—Å—Ç—ä–ø–Ω–∞.")
        st.stop()

    uploaded_img_file = st.file_uploader(
        "–ò–∑–±–µ—Ä–µ—Ç–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ...", type=["jpg", "jpeg", "png"], key="deepface_image_uploader"
    )

    if uploaded_img_file is not None:
        file_bytes = np.asarray(
            bytearray(uploaded_img_file.read()), dtype=np.uint8)
        img_cv2_original_bgr = cv2.imdecode(file_bytes, cv2.IMREAD_COLOR)

        if img_cv2_original_bgr is None:
            st.error(
                "–ù–µ –º–æ–∂–∞—Ö –¥–∞ –ø—Ä–æ—á–µ—Ç–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ—Ç–æ. –ú–æ–ª—è, –æ–ø–∏—Ç–∞–π—Ç–µ —Å –¥—Ä—É–≥ —Ñ–∞–π–ª.")
        else:
            st.subheader("–û—Ä–∏–≥–∏–Ω–∞–ª–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ:")
            img_display_resized_bgr = resize_image_for_display(
                img_cv2_original_bgr, MAX_DISPLAY_DIM)
            img_display_resized_rgb = cv2.cvtColor(
                img_display_resized_bgr, cv2.COLOR_BGR2RGB)
            st.image(img_display_resized_rgb,
                     caption="–û—Ä–∏–≥–∏–Ω–∞–ª–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ (–æ—Ä–∞–∑–º–µ—Ä–µ–Ω–æ)", use_column_width='auto')

            if st.button("üöÄ –ê–Ω–∞–ª–∏–∑–∏—Ä–∞–π –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ—Ç–æ (DeepFace)"):
                with st.spinner("–ê–Ω–∞–ª–∏–∑–∏—Ä–∞–Ω–µ –Ω–∞ –ª–∏—Ü–∞—Ç–∞ —Å DeepFace..."):
                    try:
                        face_analysis_results = DeepFace.analyze(
                            img_path=img_cv2_original_bgr.copy(),
                            actions=['age', 'gender', 'emotion'],
                            # False, –∑–∞ –¥–∞ –Ω–µ –≥—ä—Ä–º–∏, –∞–∫–æ –Ω—è–º–∞ –ª–∏—Ü–µ, –Ω–æ –ø–∞–∫ –¥–∞–≤–∞ —Ä–µ–∑—É–ª—Ç–∞—Ç.
                            enforce_detection=False,
                            # –ú–æ–∂–µ –¥–∞ —Å—Ç–∞–Ω–µ –∏–∑–±–∏—Ä–∞–µ–º, –Ω–æ opencv –µ –±—ä—Ä–∑ –∏ –¥–æ–±—ä—Ä.
                            detector_backend='opencv',
                            silent=True
                        )
                    except Exception as e:
                        st.error(f"–í—ä–∑–Ω–∏–∫–Ω–∞ –≥—Ä–µ—à–∫–∞ –ø—Ä–∏ DeepFace –∞–Ω–∞–ª–∏–∑: {e}")
                        face_analysis_results = []

                img_cv2_annotated_bgr = img_cv2_original_bgr.copy()

                if not face_analysis_results or not isinstance(face_analysis_results, list):
                    st.info("–ù–µ –±—è—Ö–∞ –æ—Ç–∫—Ä–∏—Ç–∏ –ª–∏—Ü–∞ –∏–ª–∏ —Ä–µ–∑—É–ª—Ç–∞—Ç—ä—Ç –µ –Ω–µ–≤–∞–ª–∏–¥–µ–Ω.")
                else:
                    num_faces_detected = 0
                    for i, face_result in enumerate(face_analysis_results):
                        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∞–ª–∏ 'region' —Å—ä—â–µ—Å—Ç–≤—É–≤–∞ –∏ –¥–∞–ª–∏ W/H —Å–∞ –≤–∞–ª–∏–¥–Ω–∏ (—Ç.–µ. –ª–∏—Ü–µ –µ –¥–µ—Ç–µ–∫—Ç–Ω–∞—Ç–æ)
                        if 'region' not in face_result or face_result['region']['w'] == 0 or face_result['region']['h'] == 0:
                            # –ê–∫–æ DeepFace –≤—ä—Ä–Ω–µ 1 –∑–∞–ø–∏—Å –±–µ–∑ —Ä–µ–≥–∏–æ–Ω, –∑–Ω–∞—á–∏ –Ω—è–º–∞ –ª–∏—Ü–µ.
                            if len(face_analysis_results) == 1:
                                st.info("–ù–µ –±—è—Ö–∞ –æ—Ç–∫—Ä–∏—Ç–∏ –ª–∏—Ü–∞ –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ—Ç–æ.")
                            continue  # –ü—Ä–æ–ø—É—Å–∫–∞–º–µ —Ç–µ–∫—É—â–∏—è –Ω–µ–≤–∞–ª–∏–¥–µ–Ω —Ä–µ–∑—É–ª—Ç–∞—Ç

                        num_faces_detected += 1
                        st.write(f"---")
                        st.write(f"#### –õ–∏—Ü–µ {num_faces_detected}:")

                        age = face_result.get('age', 'N/A')

                        dominant_gender_map = face_result.get('gender', {})
                        if dominant_gender_map:
                            dominant_gender = max(
                                dominant_gender_map, key=dominant_gender_map.get)
                            gender_confidence = dominant_gender_map[dominant_gender]
                        # Fallback, –∞–∫–æ 'gender' –Ω–µ –µ dict (–Ω–∞–ø—Ä. —Å–∞–º–æ dominant_gender –µ –≤—ä—Ä–Ω–∞—Ç–æ)
                        else:
                            dominant_gender = face_result.get(
                                'dominant_gender', 'N/A')
                            gender_confidence = 0  # Cannot determine confidence without the map

                        dominant_emotion_map = face_result.get('emotion', {})
                        if dominant_emotion_map:
                            dominant_emotion = max(
                                dominant_emotion_map, key=dominant_emotion_map.get)
                            emotion_confidence = dominant_emotion_map[dominant_emotion]
                        else:  # Fallback, –∞–∫–æ 'emotion' –Ω–µ –µ dict
                            dominant_emotion = face_result.get(
                                'dominant_emotion', 'N/A')
                            emotion_confidence = 0  # Cannot determine confidence without the map

                        region = face_result['region']

                        st.write(f"- **–í—ä–∑—Ä–∞—Å—Ç:** {age}")
                        st.write(
                            f"- **–ü–æ–ª:** {dominant_gender} ({gender_confidence:.2f}%)")
                        st.write(
                            f"- **–ï–º–æ—Ü–∏—è:** {dominant_emotion} ({emotion_confidence:.2f}%)")

                        x, y, w, h = region['x'], region['y'], region['w'], region['h']
                        cv2.rectangle(img_cv2_annotated_bgr,
                                      (x, y), (x+w, y+h), (0, 255, 0), 2)
                        text_gender_char = dominant_gender[0].upper(
                            # Ensure gender is not N/A
                        ) if dominant_gender and isinstance(dominant_gender, str) and dominant_gender != "N/A" else "N"
                        text = f"A:{age} G:{text_gender_char} E:{dominant_emotion}"
                        cv2.putText(img_cv2_annotated_bgr, text, (x, y - 10),
                                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)

                    if num_faces_detected > 0:
                        st.subheader("–ê–Ω–æ—Ç–∏—Ä–∞–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ:")
                        annotated_display_resized_bgr = resize_image_for_display(
                            img_cv2_annotated_bgr, MAX_DISPLAY_DIM)
                        annotated_display_resized_rgb = cv2.cvtColor(
                            annotated_display_resized_bgr, cv2.COLOR_BGR2RGB)
                        st.image(annotated_display_resized_rgb,
                                 caption=f"–ê–Ω–∞–ª–∏–∑–∏—Ä–∞–Ω–∏ –ª–∏—Ü–∞: {num_faces_detected}", use_column_width='auto')
                    elif num_faces_detected == 0:
                        st.info(
                            "–ù–µ –±—è—Ö–∞ –æ—Ç–∫—Ä–∏—Ç–∏ –ª–∏—Ü–∞ –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ—Ç–æ, –∫–æ–∏—Ç–æ –¥–∞ –±—ä–¥–∞—Ç –∞–Ω–∞–ª–∏–∑–∏—Ä–∞–Ω–∏.")

# ==============================================================================
# –†–ï–ñ–ò–ú 2: –°–†–ê–í–ù–Ø–í–ê–ù–ï –ù–ê –î–í–ï –õ–ò–¶–ê (INSIGHTFACE)
# ==============================================================================
elif analysis_mode == "–°—Ä–∞–≤–Ω—è–≤–∞–Ω–µ –Ω–∞ –¥–≤–µ –ª–∏—Ü–∞ (InsightFace)":
    st.header("2. –°—Ä–∞–≤–Ω—è–≤–∞–Ω–µ –Ω–∞ –¥–≤–µ –ª–∏—Ü–∞ (Cosine Similarity —Å InsightFace)")
    if not INSIGHTFACE_AVAILABLE or insightface_model_app is None:
        st.error(
            "InsightFace –º–æ–¥–µ–ª—ä—Ç –Ω–µ –µ –∑–∞—Ä–µ–¥–µ–Ω –∏–ª–∏ –±–∏–±–ª–∏–æ—Ç–µ–∫–∞—Ç–∞ –ª–∏–ø—Å–≤–∞. –§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª–Ω–æ—Å—Ç—Ç–∞ –Ω–µ –µ –¥–æ—Å—Ç—ä–ø–Ω–∞.")
        st.stop()  # –°–ø–∏—Ä–∞–º–µ –∏–∑–ø—ä–ª–Ω–µ–Ω–∏–µ—Ç–æ, –∞–∫–æ –º–æ–¥–µ–ª—ä—Ç –Ω–µ –µ –Ω–∞–ª–∏—á–µ–Ω.

    col1, col2 = st.columns(2)
    with col1:
        uploaded_img1_file = st.file_uploader(
            "–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ 1:", type=["jpg", "jpeg", "png"], key="insightface_img1")
    with col2:
        uploaded_img2_file = st.file_uploader(
            "–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ 2:", type=["jpg", "jpeg", "png"], key="insightface_img2")

    def get_face_embedding_and_draw_insight(image_file, img_identifier_str, insight_app_model):
        if image_file is None:
            return None, None, f"–ù–µ –µ –∫–∞—á–µ–Ω —Ñ–∞–π–ª –∑–∞ –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ {img_identifier_str}"

        file_bytes_img = np.asarray(
            bytearray(image_file.read()), dtype=np.uint8)
        img_cv_original_bgr = cv2.imdecode(file_bytes_img, cv2.IMREAD_COLOR)

        if img_cv_original_bgr is None:
            st.warning(
                f"–ù–µ—É—Å–ø–µ—à–Ω–æ –∑–∞—Ä–µ–∂–¥–∞–Ω–µ –Ω–∞ –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ {img_identifier_str}.")
            return None, None, image_file.name

        faces = insight_app_model.get(img_cv_original_bgr)
        img_cv_annotated_bgr = img_cv_original_bgr.copy()

        if not faces:
            st.info(
                f"–ù–µ —Å–∞ –Ω–∞–º–µ—Ä–µ–Ω–∏ –ª–∏—Ü–∞ –Ω–∞ –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ {img_identifier_str} ({image_file.name}).")
            return None, img_cv_annotated_bgr, image_file.name

        main_face = faces[0]
        if len(faces) > 1:
            st.caption(
                f"–ù–∞–º–µ—Ä–µ–Ω–∏ {len(faces)} –ª–∏—Ü–∞ –≤ –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ {img_identifier_str}, –∏–∑–ø–æ–ª–∑–≤–∞ —Å–µ –ø—ä—Ä–≤–æ—Ç–æ.")

        bbox = main_face.bbox.astype(int)
        cv2.rectangle(img_cv_annotated_bgr,
                      (bbox[0], bbox[1]), (bbox[2], bbox[3]), (0, 255, 0), 2)
        embedding = main_face.embedding
        return embedding, img_cv_annotated_bgr, image_file.name

    if uploaded_img1_file and uploaded_img2_file:
        if st.button("üöÄ –°—Ä–∞–≤–Ω–∏ –ª–∏—Ü–∞—Ç–∞ (InsightFace)"):
            with st.spinner("–û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ç–∞ –∏ –∏–∑—á–∏—Å–ª—è–≤–∞–Ω–µ –Ω–∞ —Å—Ö–æ–¥—Å—Ç–≤–æ..."):
                embedding1, img1_processed_bgr, filename1 = get_face_embedding_and_draw_insight(
                    uploaded_img1_file, "1", insightface_model_app)
                embedding2, img2_processed_bgr, filename2 = get_face_embedding_and_draw_insight(
                    uploaded_img2_file, "2", insightface_model_app)

                col_disp1, col_disp2 = st.columns(2)
                if img1_processed_bgr is not None:
                    with col_disp1:
                        resized_img1_bgr = resize_image_for_display(
                            img1_processed_bgr, MAX_DISPLAY_DIM // 2 + 100)
                        resized_img1_rgb = cv2.cvtColor(
                            resized_img1_bgr, cv2.COLOR_BGR2RGB)
                        st.image(resized_img1_rgb,
                                 caption=f"–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ 1: {filename1}", use_column_width='auto')
                else:
                    with col_disp1:
                        st.warning(
                            f"–ü—Ä–æ–±–ª–µ–º —Å –æ–±—Ä–∞–±–æ—Ç–∫–∞—Ç–∞ –Ω–∞ –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ 1: {filename1}")

                if img2_processed_bgr is not None:
                    with col_disp2:
                        resized_img2_bgr = resize_image_for_display(
                            img2_processed_bgr, MAX_DISPLAY_DIM // 2 + 100)
                        resized_img2_rgb = cv2.cvtColor(
                            resized_img2_bgr, cv2.COLOR_BGR2RGB)
                        st.image(resized_img2_rgb,
                                 caption=f"–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ 2: {filename2}", use_column_width='auto')
                else:
                    with col_disp2:
                        st.warning(
                            f"–ü—Ä–æ–±–ª–µ–º —Å –æ–±—Ä–∞–±–æ—Ç–∫–∞—Ç–∞ –Ω–∞ –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ 2: {filename2}")

                if embedding1 is not None and embedding2 is not None:
                    similarity_score = cosine_similarity(
                        embedding1.reshape(1, -1), embedding2.reshape(1, -1))[0][0]
                    st.subheader(f"–†–µ–∑—É–ª—Ç–∞—Ç –æ—Ç —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ—Ç–æ:")
                    st.metric(label="Cosine Similarity",
                              value=f"{similarity_score:.4f}")
                    threshold = 0.58
                    if similarity_score >= threshold:
                        st.success(
                            f"–õ–∏—Ü–∞—Ç–∞ —Å–∞ –í–ï–†–û–Ø–¢–ù–û –Ω–∞ –µ–¥–∏–Ω –∏ —Å—ä—â–∏ —á–æ–≤–µ–∫ (—Å—Ö–æ–¥—Å—Ç–≤–æ >= {threshold}).")
                    else:
                        st.warning(
                            f"–õ–∏—Ü–∞—Ç–∞ —Å–∞ –í–ï–†–û–Ø–¢–ù–û –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω–∏ —Ö–æ—Ä–∞ (—Å—Ö–æ–¥—Å—Ç–≤–æ < {threshold}).")
                    st.caption(
                        f"(–ò–∑–ø–æ–ª–∑–≤–∞–Ω –ø—Ä–∞–≥: {threshold} –∑–∞ –º–æ–¥–µ–ª 'buffalo_l'. –°—Ç–æ–π–Ω–æ—Å—Ç–∏—Ç–µ —Å–∞ –º–µ–∂–¥—É -1 –∏ 1. –ü–æ-–≤–∏—Å–æ–∫–∞ = –ø–æ-–≥–æ–ª—è–º–æ —Å—Ö–æ–¥—Å—Ç–≤–æ.)")
                elif (uploaded_img1_file and uploaded_img2_file) and (embedding1 is None or embedding2 is None):
                    st.error(
                        "–ù–µ –º–æ–∂–∞ –¥–∞ —Å–µ –∏–∑–≤–ª–µ—á–µ –ª–∏—Ü–µ –æ—Ç –µ–¥–Ω–æ—Ç–æ –∏–ª–∏ –¥–≤–µ—Ç–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ—Ç–æ –µ –Ω–µ–≤—ä–∑–º–æ–∂–Ω–æ.")


# ==============================================================================
# –†–ï–ñ–ò–ú 3: –ê–ù–ê–õ–ò–ó –ù–ê –ï–ú–û–¶–ò–ò –í–™–í –í–ò–î–ï–û (DEEPFACE)
# ==============================================================================
elif analysis_mode == "–ê–Ω–∞–ª–∏–∑ –Ω–∞ –µ–º–æ—Ü–∏–∏ –≤—ä–≤ –≤–∏–¥–µ–æ (DeepFace)":
    st.header("3. –ê–Ω–∞–ª–∏–∑ –Ω–∞ –µ–º–æ—Ü–∏–∏ –≤—ä–≤ –≤–∏–¥–µ–æ (DeepFace)")
    if not deepface_models_loaded:
        st.error("DeepFace –º–æ–¥–µ–ª–∏—Ç–µ –Ω–µ —Å–∞ –∑–∞—Ä–µ–¥–µ–Ω–∏. –§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª–Ω–æ—Å—Ç—Ç–∞ –Ω–µ –µ –¥–æ—Å—Ç—ä–ø–Ω–∞.")
        st.stop()

    uploaded_video_file = st.file_uploader(
        "–ò–∑–±–µ—Ä–µ—Ç–µ –≤–∏–¥–µ–æ —Ñ–∞–π–ª (.mp4, .avi, .mov, .mkv)...", type=["mp4", "avi", "mov", "mkv"], key="deepface_video_uploader"
    )

    frame_skip_file = st.sidebar.slider(
        "–ü—Ä–æ–ø—É—Å–∫–∞–π –∫–∞–¥—Ä–∏ –∑–∞ –∞–Ω–∞–ª–∏–∑ (—Ñ–∞–π–ª):", 0, 10, 1, key="video_frame_skip_file")
    detector_backend_video_file = st.sidebar.selectbox(
        "DeepFace –¥–µ—Ç–µ–∫—Ç–æ—Ä –∑–∞ –≤–∏–¥–µ–æ (—Ñ–∞–π–ª):",
        ('opencv', 'ssd', 'mtcnn', 'retinaface', 'yunet'), index=0, key="video_detector_file")

    # –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –Ω–∞ –≤—Ä–µ–º–µ–Ω–Ω–∏—Ç–µ —Ñ–∞–π–ª–æ–≤–µ –∑–∞ –≤—Ö–æ–¥–Ω–æ –∏ –∏–∑—Ö–æ–¥–Ω–æ –≤–∏–¥–µ–æ
    # –ò–∑—á–∏—Å—Ç–≤–∞–Ω–µ –Ω–∞ —Å—Ç–∞—Ä–∏ –≤—Ä–µ–º–µ–Ω–Ω–∏ —Ñ–∞–π–ª–æ–≤–µ, –∞–∫–æ –µ –∫–∞—á–µ–Ω –Ω–æ–≤ –≤–∏–¥–µ–æ—Ñ–∞–π–ª
    if uploaded_video_file is not None:
        if 'temp_video_path' not in st.session_state or st.session_state.get('last_uploaded_video_name') != uploaded_video_file.name:
            # –ü—Ä–µ–º–∞—Ö–≤–∞–º–µ —Å—Ç–∞—Ä–∏—è –≤—Ä–µ–º–µ–Ω–µ–Ω –≤–∏–¥–µ–æ —Ñ–∞–π–ª, –∞–∫–æ —Å—ä—â–µ—Å—Ç–≤—É–≤–∞
            if 'temp_video_path' in st.session_state and os.path.exists(st.session_state.temp_video_path):
                try:
                    os.remove(st.session_state.temp_video_path)
                    del st.session_state.temp_video_path
                except Exception as e:
                    st.warning(
                        f"–ù–µ –º–æ–≥–∞ –¥–∞ –∏–∑—Ç—Ä–∏—è —Å—Ç–∞—Ä–∏—è –≤—Ö–æ–¥–µ–Ω –≤–∏–¥–µ–æ —Ñ–∞–π–ª: {e}")
            # –ü—Ä–µ–º–∞—Ö–≤–∞–º–µ —Å—Ç–∞—Ä–∏—è –≤—Ä–µ–º–µ–Ω–µ–Ω –∏–∑—Ö–æ–¥–µ–Ω –≤–∏–¥–µ–æ —Ñ–∞–π–ª, –∞–∫–æ —Å—ä—â–µ—Å—Ç–≤—É–≤–∞
            if 'temp_output_video_path' in st.session_state and os.path.exists(st.session_state.temp_output_video_path):
                try:
                    os.remove(st.session_state.temp_output_video_path)
                    del st.session_state.temp_output_video_path
                except Exception as e:
                    st.warning(
                        f"–ù–µ –º–æ–≥–∞ –¥–∞ –∏–∑—Ç—Ä–∏—è —Å—Ç–∞—Ä–∏—è –∏–∑—Ö–æ–¥–µ–Ω –≤–∏–¥–µ–æ —Ñ–∞–π–ª: {e}")

            # –ó–∞–ø–∏—Å–≤–∞–º–µ –Ω–æ–≤–∏—è –∫–∞—á–µ–Ω —Ñ–∞–π–ª –≤—ä–≤ –≤—Ä–µ–º–µ–Ω–µ–Ω —Ñ–∞–π–ª
            with tempfile.NamedTemporaryFile(delete=False, suffix=f".{uploaded_video_file.name.split('.')[-1]}") as tfile:
                tfile.write(uploaded_video_file.read())
                st.session_state.temp_video_path = tfile.name
                st.session_state.last_uploaded_video_name = uploaded_video_file.name

        temp_video_path = st.session_state.temp_video_path

        st.subheader("–û—Ä–∏–≥–∏–Ω–∞–ª–Ω–æ –≤–∏–¥–µ–æ:")
        col_orig_video, _ = st.columns([2, 1])
        with col_orig_video:
            if os.path.exists(temp_video_path):
                st.video(temp_video_path)
            else:
                st.warning(
                    "–û—Ä–∏–≥–∏–Ω–∞–ª–Ω–∏—è—Ç –≤–∏–¥–µ–æ —Ñ–∞–π–ª –Ω–µ –µ –Ω–∞–º–µ—Ä–µ–Ω. –ú–æ–ª—è, –∫–∞—á–µ—Ç–µ –≥–æ –æ—Ç–Ω–æ–≤–æ.")
        st.caption(f"–û—Ä–∏–≥–∏–Ω–∞–ª–Ω–æ—Ç–æ –≤–∏–¥–µ–æ —Å–µ –ø–æ–∫–∞–∑–≤–∞ —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –æ—Ä–∞–∑–º–µ—Ä—è–≤–∞–Ω–µ.")

        if st.button("üöÄ –ê–Ω–∞–ª–∏–∑–∏—Ä–∞–π –≤–∏–¥–µ–æ—Ç–æ (DeepFace)", key="analyze_video_button"):
            if not os.path.exists(temp_video_path):
                st.error(
                    "–ì—Ä–µ—à–∫–∞: –í—Ö–æ–¥–Ω–∏—è—Ç –≤–∏–¥–µ–æ —Ñ–∞–π–ª –Ω–µ –µ –Ω–∞–º–µ—Ä–µ–Ω. –ú–æ–ª—è, –∫–∞—á–µ—Ç–µ –≥–æ –æ—Ç–Ω–æ–≤–æ.")
                st.stop()

            base_name, ext_name = os.path.splitext(uploaded_video_file.name)
            output_video_filename_for_download = f"{base_name}_emotions_processed.mp4"

            # Check if an output video for *this* input file has already been generated
            if 'temp_output_video_path' in st.session_state and os.path.exists(st.session_state.temp_output_video_path):
                if st.session_state.get('last_processed_video_base_name') != base_name:
                    try:
                        os.remove(st.session_state.temp_output_video_path)
                        del st.session_state.temp_output_video_path  # Clean up
                    except Exception:
                        pass  # Ignore if removal fails, might be in use

            if 'temp_output_video_path' not in st.session_state or not os.path.exists(st.session_state.get('temp_output_video_path', '')):
                st.session_state.temp_output_video_path = tempfile.NamedTemporaryFile(
                    delete=False, suffix="_processed.mp4").name
                st.session_state.last_processed_video_base_name = base_name
            temp_output_video_path = st.session_state.temp_output_video_path

            cap = None  # Initialize cap to None
            out_writer = None  # Initialize out_writer to None
            try:
                cap = cv2.VideoCapture(temp_video_path)
                if not cap.isOpened():
                    st.error(
                        f"–ì—Ä–µ—à–∫–∞: –ù–µ –º–æ–∂–µ –¥–∞ —Å–µ –æ—Ç–≤–æ—Ä–∏ –≤–∏–¥–µ–æ —Ñ–∞–π–ª—ä—Ç: {uploaded_video_file.name}")
                    st.stop()

                frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
                frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
                fps = cap.get(cv2.CAP_PROP_FPS)
                if fps == 0 or fps is None or fps > 200:
                    st.warning(
                        f"–ù–µ–≤–∞–ª–∏–¥–Ω–∞ FPS —Å—Ç–æ–π–Ω–æ—Å—Ç ({fps}) –æ—Ç –≤–∏–¥–µ–æ—Ç–æ. –ó–∞–¥–∞–≤–∞–º 25.0 FPS.")
                    fps = 25.0

                # –ü—Ä–µ–ø–æ—Ä—ä—á–∏—Ç–µ–ª–Ω–æ –∑–∞ .mp4 –∏–∑—Ö–æ–¥
                fourcc = cv2.VideoWriter_fourcc(*'mp4v')

                out_writer = cv2.VideoWriter(
                    temp_output_video_path, fourcc, fps, (frame_width, frame_height))

                if not out_writer.isOpened():
                    st.error(f"–ì—Ä–µ—à–∫–∞: –ù–µ –º–æ–∂–µ –¥–∞ —Å–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–∞ VideoWriter —Å –∫–æ–¥–µ–∫ 'mp4v'. –ü—Ä–æ–≤–µ—Ä–µ—Ç–µ –≤–∞—à–∞—Ç–∞ OpenCV/FFmpeg –∏–Ω—Å—Ç–∞–ª–∞—Ü–∏—è –∏ –Ω–∞–ª–∏—á–Ω–∏—Ç–µ –∫–æ–¥–µ—Ü–∏ (–Ω–∞–ø—Ä. gstreamer –ø–ª—ä–≥–∏–Ω–∏ –∏–ª–∏ ffmpeg —Å –ø–æ–¥—Ö–æ–¥—è—â–∞ –ø–æ–¥–¥—Ä—ä–∂–∫–∞).")
                    st.stop()

                st.info(
                    f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–∞ –≤–∏–¥–µ–æ: {uploaded_video_file.name} ({frame_width}x{frame_height} @ {fps:.2f} FPS) —Å –¥–µ—Ç–µ–∫—Ç–æ—Ä '{detector_backend_video_file}'. –ö–æ–¥–µ–∫: {'MP4V'}")
                progress_bar = st.progress(0)
                status_text = st.empty()
                percent_complete_text = st.empty()

                total_frames_prop = cap.get(cv2.CAP_PROP_FRAME_COUNT)
                total_frames = int(
                    total_frames_prop) if total_frames_prop > 0 else 0

                processed_frame_count_for_analysis = 0
                current_frame_num_read = 0
                emotions_summary = {}
                start_time = time.time()
                last_known_faces_data = []  # –°—ä—Ö—Ä–∞–Ω—è–≤–∞ –¥–∞–Ω–Ω–∏ –∑–∞ –ª–∏—Ü–∞—Ç–∞ –æ—Ç –ø–æ—Å–ª–µ–¥–Ω–∏—è –∞–Ω–∞–ª–∏–∑–∏—Ä–∞–Ω –∫–∞–¥—ä—Ä

                while cap.isOpened():
                    ret, frame_bgr = cap.read()
                    if not ret:
                        break  # –ö—Ä–∞–π –Ω–∞ –≤–∏–¥–µ–æ—Ç–æ –∏–ª–∏ –≥—Ä–µ—à–∫–∞

                    current_frame_num_read += 1
                    frame_to_write = frame_bgr.copy()

                    # –ê–Ω–∞–ª–∏–∑–∏—Ä–∞–º–µ –≤—Å–µ–∫–∏ (frame_skip_file + 1)-—Ç–∏ –∫–∞–¥—ä—Ä
                    # –ê–∫–æ frame_skip_file = 0, –∞–Ω–∞–ª–∏–∑–∏—Ä–∞ —Å–µ –≤—Å–µ–∫–∏ –∫–∞–¥—ä—Ä (0+1=1)
                    # –ê–∫–æ frame_skip_file = 1, –∞–Ω–∞–ª–∏–∑–∏—Ä–∞ —Å–µ –≤—Å–µ–∫–∏ –≤—Ç–æ—Ä–∏ –∫–∞–¥—ä—Ä (1+1=2)
                    if current_frame_num_read % (frame_skip_file + 1) == 1:
                        processed_frame_count_for_analysis += 1
                        try:
                            # DeepFace –æ—á–∞–∫–≤–∞ enforce_detection=True, –∞–∫–æ –∏—Å–∫–∞—Ç–µ –¥–∞ –≤—Ä—ä—â–∞ —Å–∞–º–æ –∑–∞—Å–µ—á–µ–Ω–∏—Ç–µ –ª–∏—Ü–∞ —Å —Ä–µ–≥–∏–æ–Ω–∏.
                            results = DeepFace.analyze(frame_bgr.copy(),
                                                       actions=['emotion'],
                                                       # –ó–∞ –¥–∞ —Å–µ –≥–∞—Ä–∞–Ω—Ç–∏—Ä–∞, —á–µ —â–µ —Å–µ –≤—ä—Ä–Ω–µ —Ä–µ–≥–∏–æ–Ω –Ω–∞ –ª–∏—Ü–µ, –∞–∫–æ –µ –æ—Ç–∫—Ä–∏—Ç–æ.
                                                       enforce_detection=True,
                                                       detector_backend=detector_backend_video_file,
                                                       silent=True)

                            current_faces_data = []
                            if isinstance(results, list) and len(results) > 0:
                                for result in results:
                                    # –£–≤–µ—Ä–µ—Ç–µ —Å–µ, —á–µ 'region' —Å—ä—â–µ—Å—Ç–≤—É–≤–∞ –∏ –∏–º–∞ –≤–∞–ª–∏–¥–Ω–∏ —Ä–∞–∑–º–µ—Ä–∏
                                    if 'region' in result and result['region']['w'] > 0 and result['region']['h'] > 0:
                                        x, y, w, h = result['region']['x'], result['region'][
                                            'y'], result['region']['w'], result['region']['h']
                                        dominant_emotion_map = result.get(
                                            'emotion', {})
                                        if dominant_emotion_map:
                                            emotion = max(
                                                dominant_emotion_map, key=dominant_emotion_map.get)
                                            confidence = dominant_emotion_map[emotion]
                                        else:
                                            emotion = result.get(
                                                'dominant_emotion', "N/A")
                                            confidence = 0.0  # –ù—è–º–∞ —Å—Ç–æ–π–Ω–æ—Å—Ç –∑–∞ —É–≤–µ—Ä–µ–Ω–æ—Å—Ç, –∞–∫–æ –ª–∏–ø—Å–≤–∞ –∫–∞—Ä—Ç–∞—Ç–∞
                                        emotions_summary[emotion] = emotions_summary.get(
                                            emotion, 0) + 1
                                        current_faces_data.append(
                                            {'box': (x, y, w, h), 'text': f"{emotion} ({confidence:.2f})"})
                            if current_faces_data:  # –ê–∫–æ –ª–∏—Ü–∞ —Å–∞ –∑–∞—Å–µ—á–µ–Ω–∏ –≤ —Ç–µ–∫—É—â–∏—è –∞–Ω–∞–ª–∏–∑–∏—Ä–∞–Ω –∫–∞–¥—ä—Ä, –∞–∫—Ç—É–∞–ª–∏–∑–∏—Ä–∞–º–µ last_known_faces_data
                                last_known_faces_data = current_faces_data
                            else:  # –ê–∫–æ –Ω—è–º–∞ –∑–∞—Å–µ—á–µ–Ω–∏ –ª–∏—Ü–∞ –≤ —Ç–µ–∫—É—â–∏—è –∞–Ω–∞–ª–∏–∑–∏—Ä–∞–Ω –∫–∞–¥—ä—Ä, –∏–∑—á–∏—Å—Ç–≤–∞–º–µ last_known_faces_data
                                last_known_faces_data = []
                        # DeepFace —Ö–≤—ä—Ä–ª—è ValueError, –∞–∫–æ enforce_detection=True –∏ –Ω–µ –µ –æ—Ç–∫—Ä–∏—Ç–æ –ª–∏—Ü–µ.
                        except ValueError:
                            last_known_faces_data = []  # –ù—è–º–∞ –ª–∏—Ü–∞ –∑–∞ —á–µ—Ä—Ç–∞–Ω–µ
                            pass  # –ü—Ä–æ–¥—ä–ª–∂–∞–≤–∞–º–µ –º—ä–ª—á–∞–ª–∏–≤–æ
                        except Exception as deepface_e:  # –•–≤–∞—â–∞–º–µ –¥—Ä—É–≥–∏ –≥—Ä–µ—à–∫–∏ –æ—Ç DeepFace
                            st.warning(
                                f"DeepFace –≥—Ä–µ—à–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑ –Ω–∞ –∫–∞–¥—ä—Ä {current_frame_num_read}: {deepface_e}")
                            last_known_faces_data = []  # –ù—è–º–∞ –ª–∏—Ü–∞ –∑–∞ —á–µ—Ä—Ç–∞–Ω–µ
                            pass  # –ü—Ä–æ–¥—ä–ª–∂–∞–≤–∞–º–µ –º—ä–ª—á–∞–ª–∏–≤–æ

                    # –†–∏—Å—É–≤–∞–º–µ –∫—É—Ç–∏–∏ –∏ —Ç–µ–∫—Å—Ç –≤—ä–∑ –æ—Å–Ω–æ–≤–∞ –Ω–∞ last_known_faces_data (–¥–æ—Ä–∏ –∏ –∞–∫–æ –∞–Ω–∞–ª–∏–∑—ä—Ç –µ –±–∏–ª –ø—Ä–æ–ø—É—Å–Ω–∞—Ç –∑–∞ —Ç–æ–∑–∏ –∫–∞–¥—ä—Ä)
                    if last_known_faces_data:
                        for face_data in last_known_faces_data:
                            x, y, w, h = face_data['box']
                            text = face_data['text']
                            cv2.rectangle(frame_to_write, (x, y),
                                          (x+w, y+h), (0, 255, 0), 2)
                            cv2.putText(frame_to_write, text, (x, y-10),
                                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
                    out_writer.write(frame_to_write)

                    if total_frames > 0:
                        progress_val = int(
                            (current_frame_num_read / total_frames) * 100)
                        progress_bar.progress(min(progress_val, 100))
                        percent_complete_text.text(
                            f"{min(progress_val, 100)}% –∑–∞–≤—ä—Ä—à–µ–Ω–æ")
                    status_text.text(
                        f"–ü—Ä–æ—á–µ—Ç–µ–Ω –∫–∞–¥—ä—Ä: {current_frame_num_read} / {total_frames if total_frames > 0 else 'N/A'}. –ê–Ω–∞–ª–∏–∑–∏—Ä–∞–Ω–∏: {processed_frame_count_for_analysis}")

                end_time = time.time()
                progress_bar.empty()
                status_text.empty()
                percent_complete_text.empty()

                st.success(
                    f"–í–∏–¥–µ–æ –æ–±—Ä–∞–±–æ—Ç–∫–∞—Ç–∞ –ø—Ä–∏–∫–ª—é—á–∏ –∑–∞ {end_time - start_time:.2f} —Å–µ–∫—É–Ω–¥–∏.")
                st.info(
                    f"–û–±—â–æ –ø—Ä–æ—á–µ—Ç–µ–Ω–∏ –∫–∞–¥—Ä–∏: {current_frame_num_read}. –ê–Ω–∞–ª–∏–∑–∏—Ä–∞–Ω–∏: {processed_frame_count_for_analysis}")

                if os.path.exists(temp_output_video_path) and os.path.getsize(temp_output_video_path) > 0:
                    st.subheader("–û–±—Ä–∞–±–æ—Ç–µ–Ω–æ –≤–∏–¥–µ–æ:")
                    col_proc_video, _ = st.columns([2, 1])
                    with col_proc_video:
                        st.video(temp_output_video_path)

                    with open(temp_output_video_path, 'rb') as video_file_for_download:
                        video_bytes_for_download = video_file_for_download.read()

                    st.download_button(
                        label="üì• –°–≤–∞–ª–∏ –æ–±—Ä–∞–±–æ—Ç–µ–Ω–æ—Ç–æ –≤–∏–¥–µ–æ",
                        data=video_bytes_for_download,
                        file_name=output_video_filename_for_download,
                        mime="video/mp4",
                        key="download_processed_video"
                    )
                else:
                    st.error(
                        "–ù–µ—É—Å–ø–µ—à–Ω–æ –≥–µ–Ω–µ—Ä–∏—Ä–∞–Ω–µ –Ω–∞ –∏–∑—Ö–æ–¥–Ω–æ—Ç–æ –≤–∏–¥–µ–æ –∏–ª–∏ —Ñ–∞–π–ª—ä—Ç –µ –ø—Ä–∞–∑–µ–Ω.")

                if emotions_summary:
                    st.subheader(
                        "–û–±–æ–±—â–µ–Ω–∏–µ –Ω–∞ –∑–∞—Å–µ—á–µ–Ω–∏—Ç–µ –¥–æ–º–∏–Ω–∏—Ä–∞—â–∏ –µ–º–æ—Ü–∏–∏ (–≤ –∞–Ω–∞–ª–∏–∑–∏—Ä–∞–Ω–∏—Ç–µ –∫–∞–¥—Ä–∏):")
                    sorted_emotions = dict(
                        sorted(emotions_summary.items(), key=lambda item: item[1], reverse=True))
                    st.bar_chart(sorted_emotions)
                else:
                    st.info(
                        "–ù–µ —Å–∞ –∑–∞—Å–µ—á–µ–Ω–∏ –µ–º–æ—Ü–∏–∏ –ø–æ –≤—Ä–µ–º–µ –Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∞—Ç–∞ (–∏–ª–∏ –Ω–µ —Å–∞ –Ω–∞–º–µ—Ä–µ–Ω–∏ –ª–∏—Ü–∞).")

            except Exception as e_video:
                st.error(
                    f"–í—ä–∑–Ω–∏–∫–Ω–∞ –Ω–µ–æ—á–∞–∫–≤–∞–Ω–∞ –≥—Ä–µ—à–∫–∞ –ø–æ –≤—Ä–µ–º–µ –Ω–∞ –≤–∏–¥–µ–æ –æ–±—Ä–∞–±–æ—Ç–∫–∞—Ç–∞: {e_video}")
                import traceback
                st.error(traceback.format_exc())
            finally:
                # –û—Å–≤–æ–±–æ–∂–¥–∞–≤–∞–Ω–µ –Ω–∞ —Ä–µ—Å—É—Ä—Å–∏—Ç–µ –Ω–∞ –∫–∞–º–µ—Ä–∞—Ç–∞ –∏ –ø–∏—Å–∞—á–∞
                if cap is not None:
                    cap.release()
                if out_writer is not None:
                    out_writer.release()
                # –ü–æ—á–∏—Å—Ç–≤–∞–Ω–µ –Ω–∞ –≤—Ä–µ–º–µ–Ω–Ω–∏—è –≤—Ö–æ–¥–µ–Ω –≤–∏–¥–µ–æ —Ñ–∞–π–ª
                if 'temp_video_path' in st.session_state and os.path.exists(st.session_state.temp_video_path):
                    try:
                        os.remove(st.session_state.temp_video_path)
                        del st.session_state.temp_video_path
                        if 'last_uploaded_video_name' in st.session_state:
                            del st.session_state.last_uploaded_video_name
                    except Exception as e:
                        st.warning(
                            f"–ù–µ –º–æ–≥–∞ –¥–∞ –∏–∑—Ç—Ä–∏—è –≤—Ä–µ–º–µ–Ω–Ω–∏—è –≤—Ö–æ–¥–µ–Ω –≤–∏–¥–µ–æ —Ñ–∞–π–ª: {e}")
                # –í—Ä–µ–º–µ–Ω–Ω–∏—è—Ç –∏–∑—Ö–æ–¥–µ–Ω –≤–∏–¥–µ–æ —Ñ–∞–π–ª –Ω–µ —Å–µ –∏–∑—Ç—Ä–∏–≤–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ —Ç—É–∫,
                # –∑–∞ –¥–∞ –º–æ–∂–µ –ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª—è—Ç –¥–∞ –≥–æ –∏–∑—Ç–µ–≥–ª–∏.

# ==============================================================================
# –†–ï–ñ–ò–ú 4: –ê–ù–ê–õ–ò–ó –û–¢ –£–ï–ë –ö–ê–ú–ï–†–ê (REAL-TIME)
# ==============================================================================
elif analysis_mode == "–ê–Ω–∞–ª–∏–∑ –æ—Ç —É–µ–± –∫–∞–º–µ—Ä–∞ (Real-time)":
    st.header("4. –ê–Ω–∞–ª–∏–∑ –æ—Ç —É–µ–± –∫–∞–º–µ—Ä–∞ (–ü–æ–ª, –í—ä–∑—Ä–∞—Å—Ç, –ï–º–æ—Ü–∏—è - Real-time)")
    st.write("–°—Ç–∞—Ä—Ç–∏—Ä–∞–π—Ç–µ —É–µ–± –∫–∞–º–µ—Ä–∞—Ç–∞ –∑–∞ –∞–Ω–∞–ª–∏–∑ –Ω–∞ –ª–∏—Ü–∞ –≤ —Ä–µ–∞–ª–Ω–æ –≤—Ä–µ–º–µ.")
    st.warning(
        "–ê–Ω–∞–ª–∏–∑—ä—Ç –≤ —Ä–µ–∞–ª–Ω–æ –≤—Ä–µ–º–µ –º–æ–∂–µ –¥–∞ –µ –±–∞–≤–µ–Ω –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç –æ—Ç –≤–∞—à–∏—è —Ö–∞—Ä–¥—É–µ—Ä.")

    if not deepface_models_loaded:
        st.error("DeepFace –º–æ–¥–µ–ª–∏—Ç–µ –Ω–µ —Å–∞ –∑–∞—Ä–µ–¥–µ–Ω–∏. –§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª–Ω–æ—Å—Ç—Ç–∞ –Ω–µ –µ –¥–æ—Å—Ç—ä–ø–Ω–∞.")
        st.stop()

    detector_backend_webcam = st.sidebar.selectbox(
        "DeepFace –¥–µ—Ç–µ–∫—Ç–æ—Ä –∑–∞ —É–µ–± –∫–∞–º–µ—Ä–∞:",
        ('opencv', 'ssd', 'mtcnn', 'retinaface', 'yunet'), index=0, key="webcam_detector")

    if 'webcam_running' not in st.session_state:
        st.session_state.webcam_running = False
    if 'cap' not in st.session_state:
        st.session_state.cap = None

    col_start, col_stop = st.columns(2)
    with col_start:
        if st.button("üöÄ –°—Ç–∞—Ä—Ç –Ω–∞ –∫–∞–º–µ—Ä–∞—Ç–∞", key="start_cam_button", disabled=st.session_state.webcam_running):
            st.session_state.webcam_running = True
            st.rerun()

    with col_stop:
        if st.button("üõë –°—Ç–æ–ø –Ω–∞ –∫–∞–º–µ—Ä–∞—Ç–∞", key="stop_cam_button", disabled=not st.session_state.webcam_running):
            st.session_state.webcam_running = False
            if st.session_state.cap is not None:
                st.session_state.cap.release()
                st.session_state.cap = None
            st.rerun()

    image_placeholder = st.empty()

    if st.session_state.webcam_running:
        if st.session_state.cap is None:
            st.session_state.cap = cv2.VideoCapture(
                0)  # 0 –∑–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞ —É–µ–± –∫–∞–º–µ—Ä–∞
            if not st.session_state.cap.isOpened():
                st.error(
                    "–ù–µ –º–æ–∂–µ –¥–∞ —Å–µ –æ—Ç–≤–æ—Ä–∏ —É–µ–± –∫–∞–º–µ—Ä–∞—Ç–∞. –ü—Ä–æ–≤–µ—Ä–µ—Ç–µ –¥–∞–ª–∏ –µ —Å–≤—ä—Ä–∑–∞–Ω–∞ –∏ –Ω–µ —Å–µ –∏–∑–ø–æ–ª–∑–≤–∞ –æ—Ç –¥—Ä—É–≥–æ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ.")
                st.session_state.webcam_running = False
                st.session_state.cap = None

        if st.session_state.cap is not None and st.session_state.cap.isOpened():
            st.info("–£–µ–± –∫–∞–º–µ—Ä–∞—Ç–∞ –µ –∞–∫—Ç–∏–≤–Ω–∞...")
            while st.session_state.webcam_running:
                ret, frame_bgr = st.session_state.cap.read()
                if not ret:
                    st.warning(
                        "–ù–µ –º–æ–∂–µ –¥–∞ —Å–µ –ø—Ä–æ—á–µ—Ç–µ –∫–∞–¥—ä—Ä –æ—Ç —É–µ–± –∫–∞–º–µ—Ä–∞—Ç–∞. –°–ø–∏—Ä–∞–º.")
                    st.session_state.webcam_running = False
                    break

                frame_to_analyze_bgr = frame_bgr.copy()
                frame_display_bgr = frame_bgr.copy()

                try:
                    # Enforce detection in webcam to ensure a face region is returned for drawing
                    results = DeepFace.analyze(
                        img_path=frame_to_analyze_bgr,
                        actions=['age', 'gender', 'emotion'],
                        enforce_detection=True,  # –ó–∞ –¥–∞ —Å–µ –≥–∞—Ä–∞–Ω—Ç–∏—Ä–∞, —á–µ —â–µ —Å–µ –≤—ä—Ä–Ω–µ —Ä–µ–≥–∏–æ–Ω –Ω–∞ –ª–∏—Ü–µ
                        detector_backend=detector_backend_webcam,
                        silent=True
                    )

                    if isinstance(results, list) and len(results) > 0:
                        for face_info in results:
                            # –û–±—Ä–∞–±–æ—Ç–≤–∞–º–µ —Å–∞–º–æ –∞–∫–æ –µ –∑–∞—Å–µ—á–µ–Ω –≤–∞–ª–∏–¥–µ–Ω —Ä–µ–≥–∏–æ–Ω –Ω–∞ –ª–∏—Ü–µ—Ç–æ
                            if 'region' in face_info and face_info['region']['w'] > 0 and face_info['region']['h'] > 0:
                                region = face_info['region']
                                x, y, w, h = region['x'], region['y'], region['w'], region['h']

                                age = face_info.get('age', "N/A")

                                gender_map = face_info.get('gender', {})
                                dominant_gender = max(gender_map, key=gender_map.get) if gender_map else face_info.get(
                                    'dominant_gender', "N/A")

                                emotion_map = face_info.get('emotion', {})
                                dominant_emotion = max(emotion_map, key=emotion_map.get) if emotion_map else face_info.get(
                                    'dominant_emotion', "N/A")

                                cv2.rectangle(frame_display_bgr, (x, y),
                                              (x + w, y + h), (0, 255, 0), 2)
                                text_gender_char = dominant_gender[0].upper(
                                ) if dominant_gender != "N/A" and isinstance(dominant_gender, str) else "N"
                                text = f"A:{age} G:{text_gender_char} E:{dominant_emotion}"
                                cv2.putText(frame_display_bgr, text, (x, y - 10),
                                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
                            elif len(results) == 1 and face_info['region']['w'] == 0:
                                # –¢–æ–≤–∞ –µ —Å–ª—É—á–∞—è—Ç, –∫–æ–≥–∞—Ç–æ DeepFace –≤—Ä—ä—â–∞ –µ–¥–∏–Ω —Ä–µ–∑—É–ª—Ç–∞—Ç, –∫–æ–π—Ç–æ –Ω–µ –µ –æ—Ç–∫—Ä–∏–ª –ª–∏—Ü–µ
                                pass
                except Exception as e:
                    # –•–≤–∞—â–∞–º–µ –≤—Å—è–∫–∞–∫–≤–∏ –≥—Ä–µ—à–∫–∏ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑ (–Ω–∞–ø—Ä. ValueError, –∞–∫–æ –Ω–µ –µ –æ—Ç–∫—Ä–∏—Ç–æ –ª–∏—Ü–µ –ø—Ä–∏ enforce_detection=True)
                    pass

                frame_display_rgb = cv2.cvtColor(
                    frame_display_bgr, cv2.COLOR_BGR2RGB)
                image_placeholder.image(frame_display_rgb, channels="RGB")

                # –ú–∞–ª–∫–æ –∑–∞–±–∞–≤—è–Ω–µ, –∑–∞ –¥–∞ –Ω–∞–º–∞–ª–∏ –Ω–∞—Ç–æ–≤–∞—Ä–≤–∞–Ω–µ—Ç–æ –Ω–∞ –ø—Ä–æ—Ü–µ—Å–æ—Ä–∞ –∏ –¥–∞ –ø–æ–∑–≤–æ–ª–∏ –Ω–∞ Streamlit –¥–∞ –∞–∫—Ç—É–∞–ª–∏–∑–∏—Ä–∞ UI
                time.sleep(0.01)

                if not st.session_state.webcam_running:
                    break  # –ò–∑–ª–∏–∑–∞–º–µ –æ—Ç —Ü–∏–∫—ä–ª–∞, –∞–∫–æ –±—É—Ç–æ–Ω—ä—Ç –∑–∞ —Å–ø–∏—Ä–∞–Ω–µ –µ –Ω–∞—Ç–∏—Å–Ω–∞—Ç

            # –û—Å–≤–æ–±–æ–∂–¥–∞–≤–∞–º–µ –∫–∞–º–µ—Ä–∞—Ç–∞ –∏ –∏–∑—á–∏—Å—Ç–≤–∞–º–µ –ø–ª–µ–π—Å—Ö–æ–ª–¥—ä—Ä–∞ —Å–ª–µ–¥ –∫—Ä–∞—è –Ω–∞ —Ü–∏–∫—ä–ª–∞
            if st.session_state.cap is not None:
                st.session_state.cap.release()
                st.session_state.cap = None
            image_placeholder.empty()
            if not st.session_state.webcam_running:
                st.info("–£–µ–± –∫–∞–º–µ—Ä–∞—Ç–∞ –µ —Å–ø—Ä—è–Ω–∞.")

        elif st.session_state.cap is None and st.session_state.webcam_running:
            # –¢–æ–∑–∏ —Å–ª—É—á–∞–π –æ–∑–Ω–∞—á–∞–≤–∞, —á–µ webcam_running –µ True, –Ω–æ cap –µ None (–Ω–µ—É—Å–ø–µ—à–Ω–æ –æ—Ç–≤–∞—Ä—è–Ω–µ)
            st.session_state.webcam_running = False  # –ù—É–ª–∏—Ä–∞–º–µ —Å—ä—Å—Ç–æ—è–Ω–∏–µ—Ç–æ
            st.error("–ì—Ä–µ—à–∫–∞: –ö–∞–º–µ—Ä–∞—Ç–∞ –Ω–µ –º–æ–∂–∞ –¥–∞ –±—ä–¥–µ —Å—Ç–∞—Ä—Ç–∏—Ä–∞–Ω–∞.")


# --- –ü—Ä–µ–º–µ—Å—Ç–µ–Ω–∏ –µ–ª–µ–º–µ–Ω—Ç–∏ –≤ –∫—Ä–∞—è –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—á–Ω–∞—Ç–∞ –ª–µ–Ω—Ç–∞ ---
st.sidebar.markdown("---")
st.sidebar.info(
    "–ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ, –±–∞–∑–∏—Ä–∞–Ω–æ –Ω–∞ [DeepFace](https://github.com/serengil/deepface), [InsightFace](https://github.com/deepinsight/insightface) –∏ [Streamlit](https://streamlit.io/).")

# --- –î–æ–±–∞–≤—è–Ω–µ –Ω–∞ –ª–æ–≥–æ—Ç–æ –≤ —Å—Ç—Ä–∞–Ω–∏—á–Ω–∞—Ç–∞ –ª–µ–Ω—Ç–∞ (sidebar) ---
LOGO_PATH = "bdu_black_logo.jpg"  # –ü–†–û–ú–Ø–ù–ê: –ü—Ä–æ–º–µ–Ω–µ–Ω–æ –∏–º–µ –Ω–∞ —Ñ–∞–π–ª–∞
LOGO_DISPLAY_WIDTH = 100  # –ú–∞–∫—Å–∏–º–∞–ª–Ω–∞ —à–∏—Ä–∏–Ω–∞ –∑–∞ –ª–æ–≥–æ—Ç–æ –≤ –ø–∏–∫—Å–µ–ª–∏ –∑–∞ –ø–æ-–º–∞–ª—ä–∫ —Ä–∞–∑–º–µ—Ä

st.sidebar.markdown("---")  # –†–∞–∑–¥–µ–ª–∏—Ç–µ–ª –ø—Ä–µ–¥–∏ –ª–æ–≥–æ—Ç–æ
if os.path.exists(LOGO_PATH):
    try:
        logo_bgr = cv2.imread(LOGO_PATH)
        if logo_bgr is not None:
            # –ò–∑–ø–æ–ª–∑–≤–∞–º–µ resize_image_for_display, –∑–∞ –¥–∞ –≥–∞—Ä–∞–Ω—Ç–∏—Ä–∞–º–µ, —á–µ NumPy –º–∞—Å–∏–≤—ä—Ç –µ —Å —Ä–∞–∑—É–º–µ–Ω —Ä–∞–∑–º–µ—Ä
            # –ø—Ä–µ–¥–∏ –¥–∞ –≥–æ –ø–æ–¥–∞–¥–µ–º –Ω–∞ Streamlit, –∏ —Å–ª–µ–¥ —Ç–æ–≤–∞ –∏–∑—Ä–∏—á–Ω–æ –∑–∞–¥–∞–≤–∞–º–µ width –∑–∞ –ø–æ–∫–∞–∑–≤–∞–Ω–µ.
            resized_logo_bgr = resize_image_for_display(
                logo_bgr, max_dim=LOGO_DISPLAY_WIDTH)
            resized_logo_rgb = cv2.cvtColor(
                resized_logo_bgr, cv2.COLOR_BGR2RGB)
            st.sidebar.image(
                resized_logo_rgb, width=LOGO_DISPLAY_WIDTH)  # –ü—Ä–µ–º–∞—Ö–Ω–∞—Ç caption —Ç—É–∫
        else:
            st.sidebar.warning(
                f"–ù–µ –º–æ–≥–∞ –¥–∞ –∑–∞—Ä–µ–¥—è –ª–æ–≥–æ—Ç–æ –æ—Ç '{LOGO_PATH}'. –ü—Ä–æ–≤–µ—Ä–µ—Ç–µ –ø—ä—Ç—è –∏–ª–∏ —Ñ–æ—Ä–º–∞—Ç–∞ –Ω–∞ —Ñ–∞–π–ª–∞.")
    except Exception as e:
        st.sidebar.error(f"–ì—Ä–µ—à–∫–∞ –ø—Ä–∏ –ø–æ–∫–∞–∑–≤–∞–Ω–µ –Ω–∞ –ª–æ–≥–æ—Ç–æ: {e}")
else:
    st.sidebar.info(
        f"–ó–∞ –¥–∞ –¥–æ–±–∞–≤–∏—Ç–µ –ª–æ–≥–æ, –ø–æ—Å—Ç–∞–≤–µ—Ç–µ —Ñ–∞–π–ª –Ω–∞ –∏–º–µ '{LOGO_PATH}' (–∏–ª–∏ –ø—Ä–æ–º–µ–Ω–µ—Ç–µ –ø—ä—Ç—è) –≤ —Å—ä—â–∞—Ç–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –∫–∞—Ç–æ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ—Ç–æ.")

# –¢–µ–∫—Å—Ç—ä—Ç "–°—ä–∑–¥–∞–¥–µ–Ω–æ –æ—Ç –µ–∫–∏–ø–∞ –Ω–∞ –ë–î–£." –ø—Ä–µ–º–µ—Å—Ç–µ–Ω —Ç—É–∫
st.sidebar.markdown("–†–∞–∑—Ä–∞–±–æ—Ç–µ–Ω–æ –æ—Ç –ö–°–¢ ‚Äì –ë—É—Ä–≥–∞—Å–∫–∏ –¥—ä—Ä–∂–∞–≤–µ–Ω —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç (–ë–î–£)")
st.sidebar.markdown("---")  # –†–∞–∑–¥–µ–ª–∏—Ç–µ–ª —Å–ª–µ–¥ –ª–æ–≥–æ—Ç–æ (–ø–æ –∂–µ–ª–∞–Ω–∏–µ)
# --- –ö—Ä–∞–π –Ω–∞ –¥–æ–±–∞–≤—è–Ω–µ—Ç–æ –Ω–∞ –ª–æ–≥–æ –∏ —Ç–µ–∫—Å—Ç ---


# --- END OF FILE second_streamlit_deepface_app.py ---
